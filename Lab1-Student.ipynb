{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D-r52JmlbM_N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 1: Introduction to Big Data Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "MMVC0DDXaWxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Introduction\n",
        "Big data processing involves techniques to efficiently handle and analyze datasets too large to fit into memory. This lab focuses on understanding partitioning, aggregation, sorting, and distributed systems concepts foundational to tools like MapReduce, Hadoop, and Spark."
      ],
      "metadata": {
        "id": "Xbx6Z8evm5Ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper: Timed Decorator\n",
        "To evaluate the execution time of each function systematically, we can create a reusable timed decorator.\n",
        "The decorator logs the execution time of any function it wraps.\n",
        "Hereâ€™s the full implementation:"
      ],
      "metadata": {
        "id": "D-r52JmlbM_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "function_perf_tracker = {}\n",
        "\n",
        "def timed(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function '{func.__name__}' executed in {end_time - start_time:.4f} seconds.\")\n",
        "        function_perf_tracker[func.__name__] = end_time - start_time\n",
        "        return result\n",
        "    return wrapper\n"
      ],
      "metadata": {
        "id": "qRoXGpNjbNY6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1: Searching via Partitioning\n",
        "Efficiently search for integers in a large dataset using both naive (linear) and optimized (partitioned) approaches.\n",
        "\n"
      ],
      "metadata": {
        "id": "lnj0aGf4ao7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Generation\n",
        "\n",
        "Use Python's random module to generate the dataset:"
      ],
      "metadata": {
        "id": "2KahqWRkaykn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g3vmOrIQaGUC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset\n",
        "with open(\"search_data.txt\", \"w\") as file:\n",
        "    for _ in range(900000):\n",
        "        file.write(f\"{random.randint(1, 1000000)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Linear Search\n",
        "\n",
        "A naive approach to scan the dataset sequentially:"
      ],
      "metadata": {
        "id": "NvxJDYUSbAw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def linear_search(filename, targets):\n",
        "    results = []\n",
        "    target_set = set(targets)\n",
        "    results = {target: \"NO\" for target in targets}\n",
        "    # TODO: Implement the linear search logic\n",
        "    # Hint: Read the file line by line, check if the number is in targets, and store \"YES\" or \"NO\"\n",
        "    with open(filename, 'r') as file:\n",
        "      for line in file:\n",
        "        num = int(line.strip())\n",
        "        if num in target_set:\n",
        "          results[num] = \"YES\"\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "targets = [5, 1000, 250000, 750000, 999999]\n",
        "print(linear_search(\"search_data.txt\", targets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJhCKiwKbC41",
        "outputId": "ca68bbc9-4976-4d18-af9f-b2017284383b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'linear_search' executed in 0.3039 seconds.\n",
            "{5: 'NO', 1000: 'NO', 250000: 'YES', 750000: 'NO', 999999: 'YES'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Partitioned Search\n",
        "\n",
        "Partition the dataset into 1,000 smaller files:"
      ],
      "metadata": {
        "id": "E8ICRpona8Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_dataset(input_file, partitions=1000):\n",
        "    # TODO: Implement dataset partitioning\n",
        "    # Hint: Write numbers to different files based on a hash function, e.g., num % partitions\n",
        "    partition_files = [open(f\"partition_{i}.txt\", \"w\") for i in range(partitions)]\n",
        "    with open(input_file, 'r') as file:\n",
        "      for line in file:\n",
        "        num = int(line.strip())\n",
        "        partition_files[num%partitions].write(line)\n",
        "\n",
        "    for pf in partition_files:\n",
        "      pf.close()\n",
        "\n",
        "partition_dataset(\"search_data.txt\")\n"
      ],
      "metadata": {
        "id": "Kez4e36Eb7VK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search in the relevant partition file:**"
      ],
      "metadata": {
        "id": "ExnrNov0cC_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def partitioned_search(partitions, targets):\n",
        "    results = []\n",
        "    target_set = set(targets)\n",
        "    results = {target: \"NO\" for target in targets}\n",
        "    # TODO: Implement partitioned search logic\n",
        "    # Hint: Open only the relevant partition file and search for the target number\n",
        "    keys = set([n%partitions for n in targets])\n",
        "    for key in keys:\n",
        "      partition_file = f\"partition_{key}.txt\"\n",
        "      with open(partition_file, 'r') as file:\n",
        "        for line in file:\n",
        "          num = int(line.strip())\n",
        "          if num in target_set:\n",
        "            results[num] = \"YES\"\n",
        "            target_set.remove(num)\n",
        "    return [results[target] for target in targets]\n",
        "\n",
        "print(partitioned_search(1000, targets))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvw1XOdncFky",
        "outputId": "5849043d-0ba2-4d1d-f39b-21d985acf0a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'partitioned_search' executed in 0.0026 seconds.\n",
            "['NO', 'NO', 'YES', 'NO', 'YES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Takeaway\n",
        "Partitioning significantly reduces the search space, mimicking distributed file systems' efficiency."
      ],
      "metadata": {
        "id": "7mXxVSxCck1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2: Grouping and Aggregation\n",
        "\n",
        "Aggregate data by grouping keys, using naive and partitioned methods."
      ],
      "metadata": {
        "id": "-HMKVGU3csJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Generation\n"
      ],
      "metadata": {
        "id": "IrLN9Jd0cz3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset\n",
        "with open(\"group_data.txt\", \"w\") as file:\n",
        "    for _ in range(30000000):\n",
        "        k = random.randint(1, 7)\n",
        "        v = random.randint(1, 1000)\n",
        "        file.write(f\"{k},{v}\\n\")\n"
      ],
      "metadata": {
        "id": "K_KsSQUscP76"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Naive Grouping\n",
        "\n",
        "Read the file and compute aggregation:"
      ],
      "metadata": {
        "id": "fS6LQ4KmdPIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "@timed\n",
        "def naive_grouping(filename):\n",
        "    aggregation = defaultdict(int)\n",
        "    # TODO: Implement naive grouping logic\n",
        "    # Hint: Read the file and aggregate values for each key (k)\n",
        "    with open(filename, 'r') as file:\n",
        "      for line in file:\n",
        "        k, v = map(int, line.strip().split(\",\"))\n",
        "        aggregation[k] += v\n",
        "    return sorted(aggregation.items())\n",
        "\n",
        "print(naive_grouping(\"group_data.txt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_0CIA2CdWDd",
        "outputId": "5f8f52ae-dc0c-4726-8976-c3b7f4a16c37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'naive_grouping' executed in 28.4005 seconds.\n",
            "[(1, 2147129101), (2, 2145185823), (3, 2144847917), (4, 2143929177), (5, 2144954441), (6, 2146209977), (7, 2143853863)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Partitioned Grouping\n",
        "\n",
        "**Partition the dataset:**\n"
      ],
      "metadata": {
        "id": "L1rbOI8qdaCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_group_data(input_file, partitions=10):\n",
        "    # TODO: Implement partitioning logic for grouping\n",
        "    # Hint: Write (k, v) pairs to files based on the hash function H(k) = k % partitions\n",
        "    partition_files = [open(f\"group_partition_{i}.txt\", \"w\") for i in range(partitions)]\n",
        "    with open(input_file, 'r') as file:\n",
        "      for line in file:\n",
        "        k, v = map(int, line.strip().split(\",\"))\n",
        "        partition_files[k%partitions].write(line)\n",
        "\n",
        "    for pf in partition_files:\n",
        "      pf.close()\n",
        "\n",
        "partition_group_data(\"group_data.txt\")"
      ],
      "metadata": {
        "id": "NWwY7C1slrCt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggregate**"
      ],
      "metadata": {
        "id": "XjlHi2QOltHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def partitioned_grouping(partitions):\n",
        "    final_aggregation = defaultdict(int)\n",
        "    all_local_aggregations = []\n",
        "    # TODO: Implement partitioned grouping logic\n",
        "    # Hint: Process each partition file and combine results\n",
        "\n",
        "    # Map : Compute local aggregations\n",
        "    for i in range(partitions):\n",
        "      local_aggregation = defaultdict(int)\n",
        "      with open(f\"group_partition_{i}.txt\", 'r') as file:\n",
        "        for line in file:\n",
        "          k, v = map(int, line.strip().split(\",\"))\n",
        "          local_aggregation[k] += v\n",
        "        all_local_aggregations.append(local_aggregation)\n",
        "\n",
        "    # Reduce : Merge local aggregations\n",
        "    for local_aggregation in all_local_aggregations:\n",
        "      for k, v in local_aggregation.items():\n",
        "        final_aggregation[k] += v\n",
        "\n",
        "    return sorted(final_aggregation.items())\n",
        "\n",
        "print(partitioned_grouping(10))\n"
      ],
      "metadata": {
        "id": "p74ie9EGdcpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a03e0bd-1f2b-4a23-e11d-d3daa5434df0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'partitioned_grouping' executed in 28.9011 seconds.\n",
            "[(1, 2147129101), (2, 2145185823), (3, 2144847917), (4, 2143929177), (5, 2144954441), (6, 2146209977), (7, 2143853863)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Takeaway\n",
        "Partitioning simulates distributed \"reduce\" operations, optimizing performance for large-scale data."
      ],
      "metadata": {
        "id": "yBiF1CTndrjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2bis: Sorting and Grouping by Key\n",
        "\n",
        "Perform grouping and aggregation on a sorted dataset by key, simulating how sorting helps optimize grouping operations in big data systems."
      ],
      "metadata": {
        "id": "j2g4KtQXxiC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Generation\n",
        "Generate the sorted dataset"
      ],
      "metadata": {
        "id": "pEWx79POxkfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def generate_sorted_group_data(file_path, size=3000000):\n",
        "    \"\"\"\n",
        "    Generate a dataset of (k, v) pairs and save it sorted by k.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): File to save the dataset.\n",
        "        size (int): Number of (k, v) pairs to generate.\n",
        "    \"\"\"\n",
        "    dataset = [(random.randint(1, 7), random.randint(1, 1000)) for _ in range(size)]\n",
        "    dataset.sort(key=lambda x: x[0])  # Sort by k\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.writelines(f\"{k},{v}\\n\" for k, v in dataset)\n",
        "    print(f\"Generated sorted dataset and saved to {file_path}.\")\n",
        "\n",
        "generate_sorted_group_data(\"sorted_group_data.txt\")"
      ],
      "metadata": {
        "id": "-z9voLPgxnXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2:  Iterator-based Grouping\n",
        "\n",
        "- Use Python iterators to simulate streaming access to the dataset file, hiding direct file handling from the processing logic.\n",
        "   - Wrap the dataset file in an iterator to process it line by line, simulating streaming access.\n",
        "   - This hides the complexity of file handling and ensures memory efficiency, especially for large datasets."
      ],
      "metadata": {
        "id": "A1iWnAHXxrco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import groupby\n",
        "\n",
        "@timed\n",
        "def iterator_based_grouping(file_path):\n",
        "    \"\"\"\n",
        "    Group and sum values by key using an iterator over a sorted dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the sorted dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: Aggregated sum of values for each key.\n",
        "    \"\"\"\n",
        "    aggregation = {}\n",
        "    #TODO: Group and sum values by key using an iterator over a sorted dataset.\n",
        "    return aggregation\n",
        "\n",
        "sorted_aggregation_data = iterator_based_grouping(\"sorted_group_data.txt\")\n",
        "sorted_aggregation_data = sorted(sorted_aggregation_data.items())\n",
        "print(sorted_aggregation_data)"
      ],
      "metadata": {
        "id": "JlTO2aW7xtzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Grouping via Iteration\n",
        "- Implement a function to iterate through the sorted dataset, grouping values by \\( k \\) as they appear. This avoids random access and simulates how sorting simplifies grouping in distributed systems.\n",
        "   - Since the file is sorted by \\( k \\), you can group values without random access:\n",
        "     1. Read the file line by line.\n",
        "     2. Accumulate values for the current key \\( k \\).\n",
        "     3. When \\( k \\) changes, save the results for the previous key and start accumulating for the new key."
      ],
      "metadata": {
        "id": "wZJuTIxsx6ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def grouping_by_iteration(file_path):\n",
        "    aggregation = {}\n",
        "    #TODO: Implement a function to iterate through the sorted dataset, grouping values by \\( k \\) as they appear.\n",
        "    return aggregation\n",
        "\n",
        "aggregation_data = grouping_by_iteration(\"sorted_group_data.txt\")\n",
        "aggregation_data = sorted(aggregation_data.items())\n",
        "print(aggregation_data)"
      ],
      "metadata": {
        "id": "JvMnDViax9Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion\n",
        "- Measure execution time for:\n",
        "  1. **Naive Grouping**: Reads and groups an unsorted file by scanning and aggregating in memory.\n",
        "  2. **Iterator-based Grouping**: Processes the sorted file line by line using the grouping-by-iteration method.\n",
        "- Compare the performance of the iterator-based grouping on the sorted file against the naive grouping on an unsorted dataset."
      ],
      "metadata": {
        "id": "i6VDiboEyTl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 3: n-way Merge-Sort\n"
      ],
      "metadata": {
        "id": "ThtdBChrgC0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Preparation\n",
        "Generate and save the sorted lists as in the original exercise.\n"
      ],
      "metadata": {
        "id": "O4WL8F2qgNRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate sorted lists and save to files\n",
        "lists = [\n",
        "    sorted(random.randint(1, 100) for _ in range(10)),\n",
        "    sorted(random.randint(50, 150) for _ in range(10)),\n",
        "    sorted(random.randint(100, 200) for _ in range(10))\n",
        "]\n",
        "\n",
        "for i, lst in enumerate(lists):\n",
        "    with open(f\"list_{i}.txt\", \"w\") as file:\n",
        "        file.writelines(f\"{x}\\n\" for x in lst)\n"
      ],
      "metadata": {
        "id": "UE87kG2igTis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Implement Pointer-Based Merge\n",
        "The idea is to read numbers from all files sequentially, maintaining a pointer (or current position) in each file to track which number should be considered next."
      ],
      "metadata": {
        "id": "BotzNulggbsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def n_way_merge_pointer(files):\n",
        "    # TODO: Implement pointer-based n-way merge logic\n",
        "    # Hint: Maintain pointers for each file and iteratively find the smallest element\n",
        "    pass\n",
        "\n",
        "# Example usage\n",
        "files = [f\"list_{i}.txt\" for i in range(3)]\n",
        "merged_output = n_way_merge_pointer(files)\n",
        "print(merged_output)\n"
      ],
      "metadata": {
        "id": "T4L-wj2rgeQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0037b7c1-504d-471e-ad5f-487eef0fec8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'n_way_merge_pointer' executed in 0.0000 seconds.\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How It Works**\n",
        "1. **Initialization:**\n",
        "\n",
        "    - Open all files and read the first line from each file to initialize the pointers.\n",
        "    - Store these first values in a pointers list.\n",
        "2. **Find the Smallest Value:**\n",
        "\n",
        "    - Use the built-in min() function to find the smallest value in the current pointers.\n",
        "3. **Update Pointers:**\n",
        "\n",
        "    - Determine which file contributed the smallest value and update its pointer by reading the next line from that file.\n",
        "4. **Repeat:**\n",
        "\n",
        "    - Continue until all files are fully read and no values remain in the pointers list.\n",
        "5. **Write Output:**\n",
        "\n",
        "    - Store the merged values in a list or write them directly to a file."
      ],
      "metadata": {
        "id": "BG3AzUvVgilB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 4: Word Counting\n",
        "Count word occurrences in a large text dataset using both naive (sequential) and partitioned (distributed) methods. This exercise simulates MapReduce-style word counting.\n"
      ],
      "metadata": {
        "id": "_E72pfzQefqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Dataset Preparation\n",
        "Prepare a large text dataset (text_data.txt). For simplicity, let's create a file with random sentences."
      ],
      "metadata": {
        "id": "80v7O3HaeqZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Generate random sentences\n",
        "def generate_text_file(filename, num_lines=1000000):\n",
        "    words = [\"apple\", \"banana\", \"orange\", \"grape\", \"pineapple\", \"kiwi\", \"melon\"]\n",
        "    with open(filename, \"w\") as file:\n",
        "        for _ in range(num_lines):\n",
        "            line = \" \".join(random.choices(words, k=random.randint(5, 15)))\n",
        "            file.write(f\"{line}\\n\")\n",
        "\n",
        "generate_text_file(\"text_data.txt\")\n"
      ],
      "metadata": {
        "id": "Rx2POwt4euS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Sequential Word Count\n",
        "Count word occurrences by scanning the file line by line."
      ],
      "metadata": {
        "id": "nkB60B0xe2p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "@timed\n",
        "def sequential_word_count(filename):\n",
        "    word_counts = defaultdict(int)\n",
        "    # TODO: Implement naive word count logic\n",
        "    # Hint: Read the file line by line and count occurrences of each word\n",
        "    return sorted(word_counts.items())\n",
        "\n",
        "# Example usage\n",
        "word_counts = sequential_word_count(\"text_data.txt\")\n",
        "print(word_counts[:10])  # Print top 10 word counts\n"
      ],
      "metadata": {
        "id": "JxGjvfyXe5JH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ae97c3-56bd-4805-9530-f5ec30f138e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'sequential_word_count' executed in 0.0000 seconds.\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Partitioned Word Count\n",
        "Use a hash function to divide the dataset into smaller files, process each partition, and combine results.\n"
      ],
      "metadata": {
        "id": "GT5VaB5We8ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partition the Dataset**\n",
        "Partition the dataset into smaller files based on a hash function."
      ],
      "metadata": {
        "id": "y52H2WCMfESW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_text_file(input_file, partitions=10):\n",
        "     # TODO: Implement partitioning logic for word count\n",
        "    # Hint: Write words to files based on the hash function H(word) = sum(ord(c) for c in word) % partitions\n",
        "    pass\n",
        "\n",
        "partition_text_file(\"text_data.txt\")\n"
      ],
      "metadata": {
        "id": "EM9wI-mGfHGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine Results**\n",
        "Aggregate word counts from all partitions."
      ],
      "metadata": {
        "id": "DtlkFzk7fPOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@timed\n",
        "def partitioned_word_count(partitions=10):\n",
        "    combined_counts = defaultdict(int)\n",
        "    # TODO: Implement partitioned word count logic\n",
        "    # Hint: Process each partition and combine the results\n",
        "    return sorted(combined_counts.items())\n",
        "\n",
        "# Example usage\n",
        "partitioned_counts = partitioned_word_count(10)\n",
        "print(partitioned_counts[:10])  # Print top 10 word counts\n"
      ],
      "metadata": {
        "id": "qPVUFGNHfR88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f980a227-ac27-481f-c5d1-431ee4bc2442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function 'partitioned_word_count' executed in 0.0000 seconds.\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Discussion\n",
        "**Sequential Approach:** Processes the entire dataset in one pass but can be slow for very large datasets due to memory constraints.  \n",
        "**Partitioned Approach:** Divides work across multiple files, simulating parallel processing and reducing memory usage. This approach is scalable and forms the basis of MapReduce-style word counting.\n"
      ],
      "metadata": {
        "id": "VDkoXrf9fTbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Key Takeaway\n",
        "The partitioned approach is more scalable for large datasets, demonstrating how the \"map\" and \"reduce\" steps in distributed frameworks like Hadoop and Spark optimize big data processing."
      ],
      "metadata": {
        "id": "7najgvy1fc5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detailed Analysis**\n",
        "\n",
        "1. **Exercise 1: Searching**\n",
        "   - **Naive Approach**: Sequentially scans the entire file for each search query, which scales poorly as \\(n\\) (number of integers) grows.\n",
        "   - **Partitioned Approach**: Limits the search to a smaller subset of the data by hashing, improving performance as \\(m\\) (number of partitions) increases.\n",
        "\n",
        "2. **Exercise 2: Grouping**\n",
        "   - **Naive Approach**: Directly aggregates values for each key in a single pass.\n",
        "   - **Partitioned Approach**: Divides the dataset into \\(m\\) smaller groups, reducing memory overhead and simulating distributed parallel processing.\n",
        "\n",
        "3. **Exercise 3: Merge-Sort**\n",
        "   - **Pointer-Based Merge**: Simpler but less efficient, as each merge step compares all \\(k\\) current elements.\n",
        "   - **`heapq` Merge**: Maintains a min-heap to quickly find the smallest element, reducing comparison overhead.\n",
        "\n",
        "4. **Exercise 4: Word Count**\n",
        "   - **Naive Approach**: Reads the dataset sequentially and counts words in memory, which becomes slow for very large datasets.\n",
        "   - **Partitioned Approach**: Uses hashing to divide data into manageable chunks, allowing efficient in-memory counting for each partition.\n"
      ],
      "metadata": {
        "id": "ALx0I5FohmHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Takeaways**\n",
        "- **Partitioning**: Improves scalability in Exercises 1, 2, and 4 by reducing the size of the data each step processes.\n",
        "- **Parallelism**: Many \"advanced\" methods simulate distributed systems, which are inherently more scalable for big data problems.\n"
      ],
      "metadata": {
        "id": "3tIOd5Hhh9Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Track"
      ],
      "metadata": {
        "id": "GZF4LXCUjfNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in function_perf_tracker.items():\n",
        "  print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "Np1A4qefjiHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c9859b-1b2f-4d67-f653-ba4b8e90e597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_search: 1.430511474609375e-06\n",
            "partitioned_search: 1.1920928955078125e-06\n",
            "naive_grouping: 8.106231689453125e-06\n",
            "partitioned_grouping: 4.0531158447265625e-06\n",
            "n_way_merge_pointer: 7.152557373046875e-07\n",
            "sequential_word_count: 6.198883056640625e-06\n",
            "partitioned_word_count: 6.4373016357421875e-06\n"
          ]
        }
      ]
    }
  ]
}