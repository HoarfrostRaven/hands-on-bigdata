{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: MapReduce with PySpark\n",
    "\n",
    "## Objectives\n",
    "In this lab, you will:\n",
    "- Learn how PySpark processes data in a distributed system.\n",
    "- Implement MapReduce operations like `map`, `reduceByKey`, and `sortBy`.\n",
    "- Explore how partitioning affects performance.\n",
    "\n",
    "### Key Exercises\n",
    "1. Word Count\n",
    "2. Grouping and Aggregation\n",
    "3. Distributed Sorting\n",
    "4. Partitioning and Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.x installed\n",
    "- PySpark installed (use `pip install pyspark` if needed)\n",
    "\n",
    "### Import Libraries\n",
    "Run the cell below to import required libraries and initialize helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timed(func):\n",
    "    \"\"\"Decorator to measure the execution time of a function.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function '{func.__name__}' executed in {end_time - start_time:.4f} seconds.\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Word Count\n",
    "\n",
    "### Objective\n",
    "Count the occurrences of each word in a large text dataset using PySpark.\n",
    "\n",
    "### Steps\n",
    "1. Load the dataset `text_data.txt`.\n",
    "2. Split each line into words.\n",
    "3. Map each word to `(word, 1)`.\n",
    "4. Use `reduceByKey` to aggregate word counts.\n",
    "5. Collect and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def word_count(file_path):\n",
    "    sc = SparkContext(\"local\", \"WordCount\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    rdd = sc.textFile(file_path)\n",
    "    \n",
    "    # Split lines into words\n",
    "    words_rdd = rdd.flatMap(lambda line: line.split())\n",
    "    \n",
    "    # Map each word to (word, 1)\n",
    "    pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "    \n",
    "    # Reduce by key to count occurrences\n",
    "    word_counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Collect and return the results\n",
    "    results = word_counts_rdd.collect()\n",
    "    sc.stop()\n",
    "    return results\n",
    "\n",
    "# Run the Word Count exercise\n",
    "results = word_count(\"text_data.txt\")\n",
    "print(results[:10])  # Print the first 10 word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Grouping and Aggregation\n",
    "\n",
    "### Objective\n",
    "Perform grouping and aggregation on a dataset of `(k, v)` pairs to compute the sum of values for each key.\n",
    "\n",
    "### Steps\n",
    "1. Load the dataset `group_data.txt`.\n",
    "2. Parse each line into `(k, v)` pairs.\n",
    "3. Use `reduceByKey` to aggregate values for each key.\n",
    "4. Collect and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def grouping_aggregation(file_path):\n",
    "    sc = SparkContext(\"local\", \"GroupingAggregation\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    rdd = sc.textFile(file_path)\n",
    "    \n",
    "    # Parse each line into (k, v) pairs\n",
    "    pairs_rdd = rdd.map(lambda line: tuple(map(int, line.split(\",\"))))\n",
    "    \n",
    "    # Use reduceByKey to sum values for each key\n",
    "    aggregated_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Collect and return the results\n",
    "    results = aggregated_rdd.collect()\n",
    "    sc.stop()\n",
    "    return results\n",
    "\n",
    "# Run the Grouping and Aggregation exercise\n",
    "grouped_results = grouping_aggregation(\"group_data.txt\")\n",
    "print(grouped_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Distributed Sorting\n",
    "\n",
    "### Objective\n",
    "Sort a dataset of integers distributed across the cluster.\n",
    "\n",
    "### Steps\n",
    "1. Load the dataset `sort_data.txt`.\n",
    "2. Parse each line into integers.\n",
    "3. Use `sortBy` to sort the dataset in ascending order.\n",
    "4. Collect and display the sorted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def distributed_sorting(file_path):\n",
    "    sc = SparkContext(\"local\", \"DistributedSorting\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    rdd = sc.textFile(file_path)\n",
    "    \n",
    "    # Parse lines into integers\n",
    "    numbers_rdd = rdd.map(lambda line: int(line.strip()))\n",
    "    \n",
    "    # Sort the numbers\n",
    "    sorted_rdd = numbers_rdd.sortBy(lambda x: x)\n",
    "    \n",
    "    # Collect and return the sorted results\n",
    "    results = sorted_rdd.collect()\n",
    "    sc.stop()\n",
    "    return results\n",
    "\n",
    "# Run the Distributed Sorting exercise\n",
    "sorted_numbers = distributed_sorting(\"sort_data.txt\")\n",
    "print(sorted_numbers[:10])  # Print the first 10 sorted numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Partitioning and Performance Analysis\n",
    "\n",
    "### Objective\n",
    "Explore how partitioning affects the performance of distributed transformations.\n",
    "\n",
    "### Steps\n",
    "1. Load the dataset `text_data.txt`.\n",
    "2. Repartition the dataset using `repartition`.\n",
    "3. Apply a transformation (e.g., split lines into words).\n",
    "4. Measure the execution time before and after repartitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def partitioning_analysis(file_path, num_partitions):\n",
    "    sc = SparkContext(\"local\", \"PartitioningAnalysis\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    rdd = sc.textFile(file_path)\n",
    "    \n",
    "    # Repartition the dataset\n",
    "    repartitioned_rdd = rdd.repartition(num_partitions)\n",
    "    \n",
    "    # Apply a transformation (split lines into words)\n",
    "    transformed_rdd = repartitioned_rdd.flatMap(lambda line: line.split())\n",
    "    \n",
    "    # Collect and return the results\n",
    "    results = transformed_rdd.collect()\n",
    "    sc.stop()\n",
    "    return results\n",
    "\n",
    "# Run the Partitioning Analysis exercise\n",
    "partitioned_results = partitioning_analysis(\"text_data.txt\", 10)\n",
    "print(partitioned_results[:10])  # Print the first 10 results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
